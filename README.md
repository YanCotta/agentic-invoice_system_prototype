# ğŸ“Š Brim Invoice Processing System

<div align="center">

[![Python](https://img.shields.io/badge/Python-3.12+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-Latest-009688.svg)](https://fastapi.tiangolo.com/)
[![Streamlit](https://img.shields.io/badge/Streamlit-Latest-FF4B4B.svg)](https://streamlit.io/)
[![LangChain](https://img.shields.io/badge/LangChain-0.2.16-33CC33.svg)](https://langchain.io/)
[![OpenAI](https://img.shields.io/badge/OpenAI-GPT4-412991.svg)](https://openai.com/)

*An intelligent invoice processing system leveraging LangChain's multi-agent workflow*

[Overview](#-overview) â€¢
[Features](#-key-features) â€¢
[Development Journey](#-development-journey) â€¢
[Architecture](#-architecture) â€¢
[Setup Guide](#-setup-guide) â€¢
[Usage](#-usage-guide) â€¢
[Progress](#-project-progress)

</div>

## ğŸ¯ Overview

A sophisticated invoice processing system that leverages LangChain's multi-agent workflow to automate extraction, validation, and purchase order (PO) matching. Built as a technical challenge for Brim's Agentic AI Engineer position, this solution aims to reduce manual processing time by over 75% while maintaining high accuracy through intelligent error handling and human-in-the-loop review processes.

## ğŸ“‹ Key Features

- **Intelligent Processing Pipeline**
  - Processes PDFs from:
    - `data/raw/invoices/` (35 invoices)
  - Multi-agent system for extraction, validation, and matching
  - RAG-based error handling with FAISS `data/raw/test_samples/` -> (5 faulty PDFs examples to reduce the need for human review)
  - Asynchronous processing with robust error management

- **User-Friendly Interface**
  - Streamlit-powered dashboard
  - Real-time processing updates
  - Interactive invoice review system
  - Performance metrics visualization (one problem encountered and not solved mentioned below)

- **Enterprise-Ready Architecture**
  - FastAPI backend infrastructure
  - Structured logging and monitoring
  - Comprehensive test coverage (CI/CD)
  - Deployment-ready configuration
  - Dockerized and with images ready in DockerHub

## ğŸ—ï¸ Architecture

### Project Structure

```plaintext
brim_invoice_streamlit/
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ main.py
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_agent.py
â”‚   â”œâ”€â”€ extractor_agent.py
â”‚   â”œâ”€â”€ fallback_agent.py
â”‚   â”œâ”€â”€ human_review_agent.py
â”‚   â”œâ”€â”€ matching_agent.py
â”‚   â””â”€â”€ validator_agent.py
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ app.py
â”‚   â””â”€â”€ review_api.py
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ logging_config.py
â”‚   â”œâ”€â”€ monitoring.py
â”‚   â”œâ”€â”€ settings.py
â”‚   
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â””â”€â”€ anomalies.json
â”‚   â”‚   â””â”€â”€ structured_invoices.json
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ invoices/ *pdfs
â”‚   â”‚   â””â”€â”€ vendor_data.csv
â”‚   â”œâ”€â”€ temp/
â”‚   â”‚   â””â”€â”€ â€¦ (temporary files)
â”‚   â””â”€â”€ test_samples/
â”‚       â””â”€â”€ â€¦ (sample faulty invoices for rag_helper.py)
â”œâ”€â”€ data_processing/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ anomaly_detection.py
â”‚   â”œâ”€â”€ confidence_scoring.py
â”‚   â”œâ”€â”€ document_parser.py
â”‚   â”œâ”€â”€ ocr_helper.py
â”‚   â”œâ”€â”€ po_matcher.py
â”‚   â”œâ”€â”€ rag_helper.py
â”‚       
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ app.py
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ invoice.py
â”‚   â”œâ”€â”€ validation_schema.py
â”‚   
â””â”€â”€ workflows/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ orchestrator.py
    

```

### Architecture Diagram (both project variants; different reps)

```plaintext
+-------------------+       +-------------------+
|   Streamlit UI    |       |    Next.js UI     |
| (Python-based)    |       | (Production-ready)|
| - Streamlit       |       | - React, Next.js  |
|   Dashboard       |       | - Tailwind CSS    |
+-------------------+       +-------------------+
           |                         |
           +-----------+-------------+
                       |
                +------+------+
                | FastAPI     |
                | Backend     |
                | - WebSocket |
                |   Support   |
                +------+------+
                       |
           +-----------+-------------+
           |                         |
+-------------------+       +-------------------+
|   Extraction      |       |   Validation      |
|   Agent           |       |   Agent           |
| - gpt-4o-mini     |       | - Pydantic Models |
| - pdfplumber      |       |                   |
| - pytesseract     |       +-------------------+
+-------------------+                |
           |                         |
           +-----------+-------------+
                       |
                +------+------+
                | PO Matching |
                |    Agent    |
                | - Fuzzy      |
                |   Matching   |
                +------+------+
                       |
                +------+------+
                | Human Review|
                |    Agent    |
                | - Confidence|
                |   < 0.9     |
                +------+------+
                       |
                +------+------+
                | Fallback    |
                |    Agent    |
                | - FAISS RAG  |
                +------+------+
                       |
                +------+------+
                | Data Storage|
                | - structured|
                |   _invoices |
                | - anomalies  |
                +------+------+
```

```mermaid
flowchart TD
    subgraph "Streamlit Frontend [Port: 8501]"
        A1[Upload PDF<br>Single or Batch] --> A2[Real-Time Progress<br>via WebSockets]
        A2 --> A3[Invoices Table<br>Vendor, Date, Total]
        A3 --> A4[Review Panel<br>Edit Flagged Data]
        A4 --> A5[Metrics Dashboard<br>Times, Error Rates]
    end

    subgraph "FastAPI Backend [Port: 8000]"
        B1[API Endpoints<br>/upload, /invoices, /update] --> B2[WebSocket<br>/ws/process_progress]
        B2 --> B3[Orchestrator<br>Coordinates Agents]
    end

    subgraph "Multi-Agent Workflow [LangChain]"
        C1[Extraction Agent<br>gpt-4o-mini, pdfplumber] --> C2[Validation Agent<br>Pydantic, Anomalies]
        C2 --> C3[PO Matching Agent<br>Fuzzy Matching, FAISS]
        C3 --> C4{Human Review<br>Confidence < 0.9?}
        C4 -->|Yes| C5[Human Review Agent<br>Manual Correction]
        C4 -->|No| C6[Processed Output]
        C5 --> C6
    end

    subgraph "Data Processing & Storage"
        D1[RAG Helper<br>Error Classification] --> C3
        D2[Raw PDFs<br>data/raw/invoices/] --> C1
        C6 --> D3[Structured JSON<br>structured_invoices.json]
        C3 --> D4[Vendor Data<br>vendor_data.csv]
        C6 --> D5[Logs & Metrics<br>monitoring.py]
    end

    %% Connections
    A1 --> B1
    A2 --> B2
    A3 --> B1
    A4 --> B1
    A5 --> B1
    B3 --> C1
    D1 --> C5
    D5 --> A5

    %% Styling
    classDef frontend fill:#FF4B4B33,stroke:#FF4B4B;
    classDef backend fill:#00968833,stroke:#009688;
    classDef agents fill:#33CC3333,stroke:#33CC33;
    classDef data fill:#41299133,stroke:#412991;
    class A1,A2,A3,A4,A5 frontend;
    class B1,B2,B3 backend;
    class C1,C2,C3,C4,C5,C6 agents;
    class D1,D2,D3,D4,D5 data;
```

## ğŸ“… Development Journey

### Week 1: Core Development

#### Day 1: Project Foundation

- ğŸ¯ **Objectives Achieved**
  - Created detailed 10-day roadmap
  - Analyzed technical requirements
  - Established project structure
  
- ğŸ”§ **Technical Setup**
  - Initialized repository
  - Installed core dependencies:
    - LangChain (0.2.16)
    - PDF processing tools
    - OCR capabilities
    - others..

  - Reserved AI tools:
    - GPT-o3-mini
    - Claude 3.5 Sonnet / 3.7 Sonnet Thinking
    - GitHub Copilot
    - Grok3

#### Day 2: Extraction & Validation

- ğŸ¯ **Objectives Achieved**
  - Built extraction pipeline
  - Implemented validation system
  
- ğŸ› ï¸ **Components Developed**
  1. **PDF Processing Pipeline**
     - Document parsing with pdfplumber
     - OCR processing with pytesseract
     - Data model implementation
  
  2. **Extraction Agent**
     - LangChain 0.2.16 integration
     - Mistral 7B implementation
     - JSON output formatting
  
  3. **Validation System**
     - Field validation
     - Anomaly detection
     - Format consistency checks

#### Day 3: Advanced Features

- ğŸ¯ **Objectives Achieved**
  - Enhanced error handling
  - Improved extraction accuracy
  
- ğŸ”¨ **Technical Implementation**
  - Integrated FAISS-based RAG
  - Added performance monitoring
  - Implemented fallback mechanisms
  - Enhanced logging system

#### Day 4: System Integration

- ğŸ¯ **Objectives Achieved**
  - Completed core functionality
  - Implemented frontend
  
- ğŸ› ï¸ **Features Added**
  - PO matching system
  - Human review interface
  - Processing pipeline
  - Streamlit dashboard

#### Day 5: System Refinement

- ğŸ¯ **Objectives Achieved**
  - Fixed critical issues
  - Enhanced reliability
  - Achieved fully functional state
  
- ğŸ”§ **Technical Fixes**
  1. **Pipeline Timing**
     - Issue: Inaccurate processing times
     - Solution: Enhanced monitoring system
  
  2. **Confidence Scoring**
     - Issue: Incorrect confidence calculations
     - Solution: Improved scoring algorithm
  
  3. **File Management**
     - Issue: PDF handling inefficiencies
     - Solution: Optimized storage system
  
  4. **Data Formatting**
     - Issue: Inconsistent data formats
     - Solution: Standardized processing

#### Day 6: More Project Refinement and Stabilization

- ğŸ¯ **Objectives Achieved**
  - Streamlined codebase by removing redundant files
  - Fixed backend startup issues
  - Enhanced API reliability
  
- ğŸ”§ **Technical Improvements**
  1. **Backend Optimization**
     - Merged review functionality into unified API
     - Updated uvicorn startup configuration
     - Simplified routing structure
  
  2. **Code Cleanup**
     - Removed redundant `api/human_review_api.py`
     - Consolidated workflow logic in `orchestrator.py`
     - Updated all API references to use port 8000

- ğŸ¯ **Dockerization Complete**
  - Fully Dockerized the Streamlit version with a multi-service setup (FastAPI backend and Streamlit frontend)
  - Utilized a single `Dockerfile` for both services, orchestrated with `docker-compose.yml`
  - Fixed healthcheck issue by adding `curl` to the `Dockerfile` for the `/api/invoices` endpoint check
  - Confirmed all core features (single/batch invoice processing, review, metrics) work in the Dockerized environment
  
- ğŸ”§ **Technical Implementation**
  - Created `Dockerfile` with `python:3.12-slim`, `tesseract-ocr`, and `curl`
  - Set up `docker-compose.yml` with separate `backend` and `streamlit` services
  - Added healthcheck with 30s `start_period` to ensure backend readiness
  - Successfully tested all functionalities in a containerized setup

- ğŸš¨ **Problems Encountered**
  - The 'View PDF' button may return 404 errors for batch-processed invoices due to filename mismatches in `data/raw/invoices/`
  - Status: Workaround implemented using metadata from JSON files

#### Day 7: Comprehensive Testing & Documentation Refinement

- ğŸ¯ **Objectives Achieved**
  - Comprehensive manual tests
  - CI/CDing
  - Refinement of documentations
  - Creation of demo video

## ğŸ”§ Setup Guide (Dockerized)

### Prerequisites

- Docker
- Docker Compose
- Git
- OpenAI API key
- Sample data files

### Setup Guide (Dockerized)

1. **Clone the repository**:

   ```bash
   git clone https://github.com/yourusername/brim_invoice_streamlit.git
   cd brim_invoice_streamlit
   ```

2. **Create Environment File**

   ```bash
   echo "OPENAI_API_KEY=your_api_key_here" > .env
   ```

3. **Verify Sample Data**
   - Confirm presence of:
   - PDFs in `data/raw/invoices/` (e.g., sample invoices)
   - Test files in `data/raw/test_samples/` (e.g., faulty invoices for RAG)
   - `data/raw/vendor_data.csv`
   - If missing, add sample PDFs and CSV as needed.

4. **Build and Run**

   ```bash
   docker compose up --build -d
   ```

   Note: curl is required in the container for the healthcheck to function.

5. **System Access**
   - Frontend: <http://localhost:8501>
   - API Endpoint: <http://localhost:8000>

6. **Optional: Use pre-built Docker images:**

Pull the images from Docker Hub:

```bash
docker pull yancotta/brim_invoice_streamlit_backend:latest
docker pull yancotta/brim_invoice_streamlit_streamlit:latest
```

Edit docker-compose.yml to use these images instead of building locally (replace the build sections with image: yancotta/brim_invoice_streamlit_backend:latest, etc.).

## ğŸš€ Usage Guide

### Processing Logic

- **Duplicate Detection**: Automatic flagging by invoice_number
- **Confidence Thresholds**: â‰¥0.9 for auto-processing, <0.9 requires human review
- **Processing Mode**: Asynchronous execution
- **Data Persistence**: Full metrics and logging

### Core Workflows

1. **Invoice Processing**
   - Upload PDFs through Streamlit interface
   - Monitor processing status
   - View extraction results

2. **Results Management**
   - View processed invoices
   - Review flagged items
   - Track performance metrics

3. **Review Process**
   - Edit flagged invoices
   - Submit corrections
   - Verify changes

## ğŸ“ˆ Project Progress

### Completed (Days 1-7)

- âœ… Multi-agent system implementation
- âœ… Streamlit frontend development
- âœ… OpenAI API integration
- âœ… RAG-based error handling
- âœ… System optimizations
- âœ… Backend stabilization and cleanup
- âœ… Final stabilization and frontend improvements
- âœ… Dockerized and implemented CI/CD
- âœ… Finished documentation and testing

## ğŸ”® Future Enhancement: Database-Backed Invoice Management

### Context

The current Streamlit system uses a file-based approach (`data/raw/invoices/`) for simplicity within the 10-day challenge. However, with 5,000 monthly invoices, a scalable database solution was considered to improve manageability and usability.

### Proposed Solution

#### Architecture Components

1. **Database Layer**
   - PostgreSQL for structured metadata storage
     - Invoice numbers, vendors, dates, totals
     - Processing status and validation results
     - File references and timestamps
   - Alternative: MongoDB for flexible document storage

2. **Object Storage**
   - AWS S3 or local file server for PDF storage
   - Secure, scalable document management
   - Built-in versioning and backup capabilities

#### Implementation Steps

1. **Database Setup** (2 days)

```sql
CREATE TABLE invoices (
    id SERIAL PRIMARY KEY,
    invoice_number VARCHAR(50) UNIQUE,
    vendor_name VARCHAR(100),
    issue_date DATE,
    total_amount DECIMAL(10,2),
    status VARCHAR(20),
    confidence_score FLOAT,
    pdf_url VARCHAR(255),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

2. **Storage Configuration** (1 day)
- S3 bucket setup with appropriate permissions
- Folder structure for invoice PDFs
- Backup and retention policies

3. **Application Updates** (1-2 days)
   - Modify `app.py` for database operations
   - Implement PDF storage and retrieval
   - Update processing pipeline for new architecture

4. **Frontend Enhancements**
   - Add advanced search capabilities
   - Implement filtering and sorting
   - Enable bulk operations
   - Improve PDF preview and download

### Benefits

- ğŸ“ˆ Scalability for thousands of invoices
- ğŸ” Enhanced search and filtering
- ğŸ”’ Improved security and access control
- ğŸ“Š Better reporting capabilities
- ğŸ”„ Reliable backup and recovery

### Why Not Implemented

Time constraints within the 10-day challenge prioritized delivering a functional system. The modular design allows future database integration without significant refactoring.

### Implementation Roadmap

1. **Phase 1**: Database Integration
   - Set up PostgreSQL
   - Migrate existing data
   - Update core processing logic

2. **Phase 2**: Storage Migration
   - Configure S3 storage
   - Move PDFs to new storage
   - Update file handling logic

3. **Phase 3**: Frontend Enhancement
   - Add database-driven features
   - Implement advanced search
   - Enhance user experience

Estimated Timeline: 4-5 days for full implementation

### Known Issue: Processing Times Displaying as 0.00 Seconds

**Problem Encountered**: During testing on February 25, 2025, I noticed that all processing times (extraction, validation, matching, review, and total time) displayed as 0.00 seconds on both the Invoices and Metrics pages of the Streamlit frontend. This persisted across multiple invoices, despite other functionalitiesâ€”such as invoice extraction, validation, matching, review, and confidence scoringâ€”working correctly.

**Troubleshooting Efforts**: I investigated the timing capture logic in `workflows/orchestrator.py` and `config/monitoring.py`. Initially, the `Monitoring.timer` context manager yielded no value, causing timing variables (e.g., `extraction_time`) to remain `None` and default to 0.0. I introduced a `TimerContext` class to capture durations via `__exit__`, and updated `process_invoice` to use `timer.duration`. However, this didnâ€™t resolve the issue because the assignments were incorrectly placed inside the `with` blocks, capturing `0.0` before the duration was set.

A subsequent fix moved these assignments outside the `with` blocks to capture the actual durations post-execution. While this approach was sound in theory, applying it inadvertently broke the system due to an indentation error: the `process_invoice` method was not properly nested under the `InvoiceProcessingWorkflow` class. This led to `AttributeError` exceptions when the backend attempted to call `workflow.process_invoice`, resulting in 500 Internal Server Errors and a non-functional system.

**Current Status**: Despite multiple attempts to correct the timing captureâ€”including verifying the `TimerContext` implementation and adjusting indentationâ€”the system ceased functioning after the latest fix. Logs indicate successful agent initialization and test sample processing, but the frontend reports "Failed to connect to the server," and invoice uploads fail. Due to time constraints within the 10-day technical challenge, Iâ€™ve decided to revert to the previous working version, where all core functionalities operate correctly except for the timing metrics.

**Resolution**: For now, the processing times remain broken, displaying 0.00 seconds. This does not impact the systemâ€™s primary invoice processing capabilities, but it limits performance monitoring accuracy. A future fix would involve correctly indenting the `process_invoice` method under the class, ensuring atomic file writes to prevent JSON corruption (noted in logs as "JSON decode error"), and re-testing the timing capture logic. Given the project timeline, Iâ€™ve prioritized a fully functional system over perfecting this metric.

**Next Steps**: Post-submission, Iâ€™d address this by isolating the timing logic in a separate test harness, ensuring proper class method scoping, and implementing robust file handling to avoid concurrency issues.

### Remaining Tasks

#### Day 7-10

- Refine Documentation and Video Demo
- Delivery

---

<div align="center">

**Built with â¤ï¸ for Brim's Technical Challenge**

</div>
